

#Overview

This project demonstrates the use of Apache Spark SQL and PySpark DataFrame APIs to process flight delay data. The dataset used is departuredelays.csv, and operations include SQL queries, temp views, schema enforcement, and data storage in multiple formats.

##Features
	â€¢	Spark SQL Queries: Conversion of SQL queries into DataFrame APIs.
	â€¢	Temp Views & Catalog: Creating temporary views and querying Spark catalog.
	â€¢	Data Transformation: Applying schema, filtering, and transforming data.
	â€¢	Data Storage: Writing outputs as JSON, LZ4-compressed JSON, and Parquet.
	â€¢	ORD Flight Extraction: Filtering flights originating from Chicago Oâ€™Hare (ORD).

##Setup & Execution
	1.	Clone the Repository:

        git clone 


	2.	Run the Script:

        spark-submit --conf spark.sql.catalogImplementation=hive FlightDelayAnalysis.py ./departuredelays.csv


	3.	Output Files:
	â€¢	output/departuredelays.json
	â€¢	output/departuredelays_lz4.json
	â€¢	output/departuredelays.parquet
	â€¢	output/orddeparturedelays.parquet

##Technologies Used
	â€¢	Apache Spark
	â€¢	PySpark
	â€¢	Hadoop (Hive Catalog)
	â€¢	Parquet, JSON Storage

##Author

Srujan Shekar Shetty ðŸš€